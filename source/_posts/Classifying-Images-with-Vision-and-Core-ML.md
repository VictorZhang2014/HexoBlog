---
title: Classifying Images with Vision and Core ML
date: 2017-10-23 16:45:58
tags: Core ML, Classifying Images
---

If you don't know the basic use of Core ML, see [this page](/2017/10/23/Getting-Started-on-CoreML-by-Three-Steps/) , on the contrary, keep going our tutorial.

Use Vision with Core ML to perform image classification.

## Overview
With the `Core ML` framework, you can use a trained machine learning model to classify input data. The Vision framework works with Core ML to apply classification models to images, and to preprocess those images to make machine learning tasks easier and more reliable.

This tutorial I'm going to show you the open source `MobileNet` model, which identifies an image using 1000 classification categories as seen in the example screenshots below.

![Official Classifying Image](/img/AI/officialClassifyImages.png)
<center>Figure - Official Test Case [Download Sample Code](https://docs-assets.developer.apple.com/published/43bca2cbbd/ClassifyingImageswithVisionandCoreML.zip)</center>

<br/>
<img src="/img/AI/FountainMobileNet.jpeg" alt="" width="320" height="568" /><img src="/img/AI/StrawberryMobileNet.jpeg" alt="" width="320" height="568" />
<img src="/img/AI/LuHanMobileNet.jpeg" alt="" width="320" height="568" /><img src="/img/AI/avatarMobileNet.jpeg" alt="" width="320" height="568" />
<center>Figure - My Test Case</center>

Now, we've seen it's completely identified in official test case, but in my test case, half of them are failed to be identified. So I can sum this example code up,the trained model is unmatured. 

But anyway, this is a good and easy step for us to going.

## Start to my performance
`Core ML` automatically generates a Swift class which is `MobileNet`, that provides easy access to your ML model. To set up a Vision request using the model, create an instance of that class and use its model property to create a `VNCoreMLRequest` object. Use the request object's completion handler to specify a method to receive results from the model after you run the request. 

** Download the Model **
[Download Model](https://docs-assets.developer.apple.com/coreml/models/MobileNet.mlmodel)
After downloaded, dragging it to your project, complete snippet code as seen below.

```
import UIKit
import CoreML
import Vision

class ViewController: UIViewController {
    
    override func touchesBegan(_ touches: Set<UITouch>, with event: UIEvent?) {
        choosePhotosAndHandle()
    }
    
    //1. Choose an image and handle it
    func choosePhotosAndHandle() {
        //Specific an image
        let imgpath = Bundle.main.path(forResource: "Strawberry", ofType: ".jpeg")
        if imgpath == nil {
            print("You specified image is nonexistent")
            return
        }
        let image = UIImage(contentsOfFile: imgpath!)!
        let orientation = CGImagePropertyOrientation(rawValue: UInt32(image.imageOrientation.rawValue))!
        guard let ciImage = CIImage(image: image) else {
            fatalError("Unable to create \(CIImage.self) from \(image).")
        }
        
        DispatchQueue.global(qos: .userInitiated).async {
            let handler = VNImageRequestHandler(ciImage: ciImage, orientation: orientation)
            do {
                try handler.perform([self.classifyRequest])
            } catch {
                print("Failed to perform classification.\n\(error.localizedDescription)")
            }
        }
    }
    
    //2.Initialized an instance of MobileNet in lazy mode
    lazy var classifyRequest: VNCoreMLRequest = {
        let mlModel = MobileNet().model
        
        do {
            let model = try VNCoreMLModel(for: mlModel)
            let request = VNCoreMLRequest(model: model) { [weak self] request, error in
                self?.processClassifications(for: request, error: error)
            }
            request.imageCropAndScaleOption = .centerCrop
            return request
        } catch {
            fatalError("Failed to load Vision ML model: \(error)")
        }
    }()
    
    // 3.This is an callback
    func processClassifications(for request: VNRequest, error: Error?) {
        DispatchQueue.main.async {
            guard let results = request.results else {
                print("Unable to classify image. \(error!.localizedDescription)")
                return
            }
            
            // The `results` will always be `VNClassificationObservation`s, as specified by the Core ML model in this project.
            let classifications = results as! [VNClassificationObservation]
            
            if classifications.isEmpty {
                print("Nothing recognized")
            } else {
                // Display top classifications ranked by confidence in the UI
                let topClassifications = classifications.prefix(2)
                let descriptions = topClassifications.map({ classification in
                    return String(format: "   (%.2f) %@", classification.confidence, classification.identifier)
                })
                
                //This is the output result
                print("Classification: \n" + descriptions.joined(separator: "\n"))
            }
        }
    }
}
```
What we will always be seen in the console window as shown below:
```
Classification: 
   (1.00) strawberry
   (0.00) trifle
```

The property of `imageCropAndScaleOption` in the class of `VNCoreMLRequest` illustration is an ML model processes input images in a fixed aspect ratio, but input images may have arbitrary aspect ratio, so Vision must scale of crop the image to fit. For best results, set a value to this property that matches the image layout the model was trained with. For the [available classification models](https://developer.apple.com/machine-learning/), the `centerCrop` option is appropriate unless noted otherwise.


[Official Classifying Images Tutorial](https://developer.apple.com/documentation/vision/classifying_images_with_vision_and_core_ml)
<br/>
<br/>

=======================================================================================================
å¦‚æœä½ è¿˜ä¸çŸ¥é“Core MLçš„åŸºæœ¬ä½¿ç”¨ï¼Œé‚£å°±å…ˆçœ‹çœ‹è¿™ä¸ª[æ•™ç¨‹](/2017/10/23/Getting-Started-on-CoreML-by-Three-Steps/) ï¼Œç›¸åï¼Œå°±ç›´æ¥å¾€ä¸‹çœ‹å§ã€‚

ä½¿ç”¨`Vision with Core ML`æ¥æ‰§è¡Œå›¾ç‰‡åˆ†ç±»ï¼ˆ`image classification`ï¼‰ã€‚

## æ¦‚è¿°
é€šè¿‡`Core ML`frameworkï¼Œä½ å¯ä»¥é€šè¿‡å·²ç»è®­ç»ƒçš„æœºå™¨æ¨¡å‹å¯¹è¾“å…¥çš„æ•°æ®ï¼ˆæä¾›çš„å›¾ç‰‡ï¼‰è¿›è¡Œåˆ†ç±»ã€‚`Vision`frameworkä¸`Core ML`ç´§å¯†ç»“åˆå·¥ä½œï¼Œåº”ç”¨äºåˆ†ç±»æ¨¡å‹åˆ°å›¾ç‰‡ï¼Œå¹¶ä¸”é¢„å¤„ç†é‚£äº›å›¾ç‰‡ï¼Œä½¿æœºå™¨å­¦ä¹ ä»»åŠ¡æ›´ç®€å•ï¼Œæ›´å¯é ã€‚

æœ¬ç« æ•™ç¨‹ï¼Œæˆ‘å°†å±•ç¤ºç»™ä½ ä¸€ä¸ªå¼€æº`MobileNet`æ¨¡å‹çš„ä½¿ç”¨ï¼Œè¿™ä¸ªæ¨¡å‹èƒ½ä½¿ç”¨1000ä¸­åˆ†ç±»ç±»å‹æ¥è¯†åˆ«å›¾ç‰‡ï¼Œå¦‚ä¸‹æˆªå›¾æ‰€ç¤ºï¼š

![å®˜æ–¹åˆ†ç±»åå›¾ç‰‡æ•ˆæœ](/img/AI/officialClassifyImages.png)
<center>å›¾ç‰‡ - å®˜æ–¹æµ‹è¯•æ¡ˆä¾‹ [ä¸‹è½½æ ·æœ¬ä»£ç ](https://docs-assets.developer.apple.com/published/43bca2cbbd/ClassifyingImageswithVisionandCoreML.zip)</center>

<br/>
<img src="/img/AI/FountainMobileNet.jpeg" alt="" width="320" height="568" /><img src="/img/AI/StrawberryMobileNet.jpeg" alt="" width="320" height="568" />
<img src="/img/AI/LuHanMobileNet.jpeg" alt="" width="320" height="568" /><img src="/img/AI/avatarMobileNet.jpeg" alt="" width="320" height="568" />
<center>å›¾ç‰‡ - æˆ‘çš„æµ‹è¯•æ¡ˆä¾‹</center>

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå®˜æ–¹çš„æµ‹è¯•æ¡ˆä¾‹çš„å›¾ç‰‡å…¨éƒ¨æ­£ç¡®è¯†åˆ«äº†ï¼Œä½†æ˜¯åœ¨æˆ‘çš„æ¡ˆä¾‹é‡Œï¼Œä¸€åŠè¢«è¯†åˆ«å‡ºæ¥äº†ï¼Œä¸€åŠå¤±è´¥äº†ã€‚ç®€å•çš„æ€»ç»“ï¼Œè¿™ä¸ªæ¨¡å‹å¹¶ä¸æˆç†Ÿã€‚

ä½†æ˜¯ï¼Œä¸ç®¡æ€ä¹ˆæ ·ï¼Œè¿™å¯¹æˆ‘ä»¬æ¥è¯´ï¼Œæ˜¯ä¸€ä¸ªç®€å•è€Œä¸”å¾ˆä¸é”™çš„å¼€å§‹ã€‚

## å¼€å§‹æˆ‘çš„è¡¨æ¼”å§ğŸ˜„
`Core ML`è‡ªåŠ¨ç”Ÿæˆ`Swift`ç±»ï¼Œè¿™ä¸ªç±»å«åš`MobileNet`ï¼Œæä¾›äº†è½»æ¾çš„è®¿é—®è¿™ä¸ªMLæ¨¡å‹ã€‚ä½¿ç”¨è¿™ä¸ªæ¨¡å‹æ¥åˆ›å»º`VNCoreMLRequest`å¯¹è±¡ï¼Œå¹¶ä¸”è®¾ç½®ç›¸å…³çš„å±æ€§ï¼Œæœ€åç»™è¿™ä¸ªè¯·æ±‚æŒ‡å®šä¸€ä¸ªå›è°ƒç”¨æ¥æ¥æ”¶æ¨¡å‹çš„è¾“å‡ºç»“æœã€‚

[ä¸‹è½½æ¨¡å‹](https://docs-assets.developer.apple.com/coreml/models/MobileNet.mlmodel)
ä¸‹è½½åï¼ŒæŠŠè¿™ä¸ªæ¨¡å‹æ‹–æ‹½åˆ°ä½ çš„é¡¹ç›®ï¼Œå®Œæ•´çš„ä»£ç å¦‚ä¸‹æ˜¾ç¤º

```
import UIKit
import CoreML
import Vision

class ViewController: UIViewController {
    
    override func touchesBegan(_ touches: Set<UITouch>, with event: UIEvent?) {
        choosePhotosAndHandle()
    }
    
    //1. é€‰æ‹©ä¸€å¼ å›¾ç‰‡å¹¶ä¸”å¤„ç†å®ƒ
    func choosePhotosAndHandle() {
        //æŒ‡å®šä¸€å¼ å›¾ç‰‡
        let imgpath = Bundle.main.path(forResource: "Strawberry", ofType: ".jpeg")
        if imgpath == nil {
            print("You specified image is nonexistent")
            return
        }
        let image = UIImage(contentsOfFile: imgpath!)!
        let orientation = CGImagePropertyOrientation(rawValue: UInt32(image.imageOrientation.rawValue))!
        guard let ciImage = CIImage(image: image) else {
            fatalError("Unable to create \(CIImage.self) from \(image).")
        }
        
        DispatchQueue.global(qos: .userInitiated).async {
            let handler = VNImageRequestHandler(ciImage: ciImage, orientation: orientation)
            do {
                try handler.perform([self.classifyRequest])
            } catch {
                print("æœªèƒ½æ‰§è¡Œåˆ†ç±».\n\(error.localizedDescription)")
            }
        }
    }
    
    //2.æ‡’åŠ è½½MobileNet
    lazy var classifyRequest: VNCoreMLRequest = {
        let mlModel = MobileNet().model
        
        do {
            let model = try VNCoreMLModel(for: mlModel)
            let request = VNCoreMLRequest(model: model) { [weak self] request, error in
                self?.processClassifications(for: request, error: error)
            }
            request.imageCropAndScaleOption = .centerCrop
            return request
        } catch {
            fatalError("Failed to load Vision ML model: \(error)")
        }
    }()
    
    // 3.è¿™æ˜¯å›è°ƒ
    func processClassifications(for request: VNRequest, error: Error?) {
        DispatchQueue.main.async {
            guard let results = request.results else {
                print("Unable to classify image. \(error!.localizedDescription)")
                return
            }
            
            // `results`æ°¸è¿œéƒ½æ˜¯`VNClassificationObservation`æ•°ç»„ï¼Œåœ¨è¿™ä¸ªé¡¹ç›®ä¸­æœ‰ML Modelç‰¹æŒ‡çš„
            let classifications = results as! [VNClassificationObservation]
            
            if classifications.isEmpty {
                print("Nothing recognized")
            } else {
                // Display top classifications ranked by confidence in the UI
                let topClassifications = classifications.prefix(2)
                let descriptions = topClassifications.map({ classification in
                    return String(format: "   (%.2f) %@", classification.confidence, classification.identifier)
                })
                
                //è¿™æ˜¯è¾“å‡ºç»“æœ
                print("Classification: \n" + descriptions.joined(separator: "\n"))
            }
        }
    }
}
```
æˆ‘ä»¬å¯ä»¥åœ¨æ§åˆ¶å°çª—å£çœ‹åˆ°å¦‚ä¸‹è¾“å‡ºï¼š
```
Classification: 
   (1.00) strawberry
   (0.00) trifle
```

åœ¨ç±»`VNCoreMLRequest`çš„å±æ€§`imageCropAndScaleOption`ï¼Œè§£é‡Šï¼šä¸€ä¸ªMLæ¨¡å‹å¤„ç†å›¾ç‰‡æ—¶æ˜¯åŸºäºå›ºå®šå®½é«˜æ¯”ï¼ˆfixed aspect ratioï¼‰çš„ï¼Œä½†æ˜¯æä¾›çš„å›¾ç‰‡çš„å®½é«˜æ¯”å¯èƒ½æ˜¯ä»»æ„çš„ï¼Œæ‰€ä»¥`Vision`å°±å¿…é¡»æŠŠå›¾ç‰‡è¿›è¡Œè£åˆ‡æˆ–è€…è§„æ ¼åŒ–åå»é€‚é…è¿™ä¸ªæ¨¡å‹æ‰€éœ€ã€‚ä¸ºäº†è¾¾åˆ°æœ€å¥½çš„æ•ˆæœï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¼šè®¾ç½®ä¸€ä¸ªå€¼æ¥åŒ¹é…å›¾ç‰‡çš„å¸ƒå±€ï¼Œè¿™æ ·çš„å›¾ç‰‡çš„å¸ƒå±€å°±æ˜¯æ¨¡å‹æ‰€éœ€è¦çš„ã€‚[æ‰€æœ‰å¯ç”¨çš„åˆ†ç±»æ¨¡å‹](https://developer.apple.com/machine-learning/)ï¼Œå‚æ•°é€‰é¡¹`centerCrop`æ˜¯è¾ƒä¸ºåˆé€‚çš„ï¼Œé™¤éæœ‰ç‰¹æ®Šè¯´æ˜ç”¨åˆ«çš„ï¼Œå¦åˆ™ï¼Œå°±æ˜¯è¿™ä¸ªã€‚


[å®˜æ–¹å›¾ç‰‡åˆ†ç±»æ•™ç¨‹](https://developer.apple.com/documentation/vision/classifying_images_with_vision_and_core_ml)
